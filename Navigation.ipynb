{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rl_lib\n",
    "from rl_lib.replay import ReplayBuffer\n",
    "from rl_lib.agent import Agent, DQNExtensions\n",
    "from rl_lib.training import dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information about the environment\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state_size = len(env_info.vector_observations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the agent\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to train the agent to navigate through the environment. The agent tries to collect many yellow bananas while avoiding blue bananas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up replay buffer\n",
    "\n",
    "\n",
    "ReplayBufferInput = {\n",
    "    \"action_size\": action_size,  # dimension of each action\n",
    "    \"buffer_size\": int(1e5),  # replay buffer size\n",
    "    \"batch_size\": 64,  # minibatch size\n",
    "    \"random_seed\": 0\n",
    "}\n",
    "\n",
    "replay_buffer = ReplayBuffer(**ReplayBufferInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up the agent \n",
    "\n",
    "AgentInput = {\n",
    "    \"state_size\": state_size, # dimension of each state\n",
    "    \"action_size\": action_size, # dimension of each action\n",
    "    \"seed\": 0, # random seed\n",
    "    \"replay_buffer\": replay_buffer, # replay buffer\n",
    "    \"gamma\": 0.99, # discount factor\n",
    "    \"tau\": 1e-3, # for soft update of target parameters\n",
    "    \"learning_rate\": 5e-4, # learning rate\n",
    "    \"update_local_every\": 2, # how often to update the local network\n",
    "    \"update_target_every\": 4, # how often to update the target network\n",
    "    \"dqn_extensions\": [DQNExtensions.DoubleDQN] # if empty, the standard DQN algorithm is used for learning.\n",
    "    \n",
    "}\n",
    "\n",
    "agent = Agent(**AgentInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "(66,)\n",
      "[0.01515152 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152\n",
      " 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152\n",
      " 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152\n",
      " 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152\n",
      " 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152\n",
      " 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152\n",
      " 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152\n",
      " 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152\n",
      " 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152\n",
      " 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152\n",
      " 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152 0.01515152]\n",
      "1.0\n",
      "1.0\n",
      "(68,)\n",
      "[0.02946776 0.0049003  0.02946776 0.02946776 0.02946776 0.00340715\n",
      " 0.00257132 0.00612866 0.00083797 0.0023188  0.02946776 0.00911185\n",
      " 0.02946776 0.00123277 0.00911052 0.00107242 0.00542627 0.02946776\n",
      " 0.02946776 0.02946776 0.02946776 0.02946776 0.00457661 0.02946776\n",
      " 0.00658366 0.02946776 0.00418661 0.02946776 0.00287991 0.0017114\n",
      " 0.00082744 0.02946776 0.02946776 0.00244472 0.02946776 0.02946776\n",
      " 0.00640808 0.0041949  0.00271351 0.00143885 0.02946776 0.02946776\n",
      " 0.00625696 0.02946776 0.00252417 0.00194238 0.02946776 0.02946776\n",
      " 0.00110416 0.0019518  0.02946776 0.02946776 0.00137038 0.00220133\n",
      " 0.00232659 0.0009286  0.02946776 0.00443429 0.00835919 0.01148831\n",
      " 0.0044977  0.00404644 0.00332056 0.00973026 0.01188567 0.01245009\n",
      " 0.02946776 0.02946776]\n",
      "1.0\n",
      "1.0\n",
      "(70,)\n",
      "[0.01096903 0.01081829 0.00638377 0.00696743 0.00290604 0.00679353\n",
      " 0.00590152 0.01340744 0.00185321 0.00532194 0.0067588  0.01986385\n",
      " 0.01658827 0.00282938 0.02090984 0.00246134 0.01201867 0.00576429\n",
      " 0.01996213 0.01487737 0.01173617 0.01630411 0.00999046 0.06763238\n",
      " 0.01511035 0.06763238 0.00960882 0.00563789 0.00660976 0.00392789\n",
      " 0.00189908 0.00859423 0.06763238 0.00561096 0.00676501 0.06763238\n",
      " 0.01470739 0.00962785 0.00622785 0.00330234 0.01027688 0.0003366\n",
      " 0.01436055 0.00959635 0.0057933  0.00445802 0.01163251 0.00114285\n",
      " 0.00253419 0.00447964 0.00486837 0.00732645 0.0031452  0.00505234\n",
      " 0.00533984 0.00213126 0.00100668 0.01017728 0.01819991 0.02636718\n",
      " 0.0103228  0.00795689 0.00762111 0.02233223 0.02466967 0.02557381\n",
      " 0.01557326 0.00291425 0.06763238 0.06763238]\n",
      "[0.34115857]\n",
      "[0.34115857]\n",
      "(72, 1)\n",
      "[[0.01645428]\n",
      " [0.01625794]\n",
      " [0.00992585]\n",
      " [0.01083336]\n",
      " [0.00451848]\n",
      " [0.01056297]\n",
      " [0.00917603]\n",
      " [0.02084665]\n",
      " [0.00288148]\n",
      " [0.00827486]\n",
      " [0.00972127]\n",
      " [0.02954931]\n",
      " [0.0242713 ]\n",
      " [0.00408632]\n",
      " [0.03251182]\n",
      " [0.00382703]\n",
      " [0.01868731]\n",
      " [0.00787215]\n",
      " [0.02956131]\n",
      " [0.02146211]\n",
      " [0.01733763]\n",
      " [0.02455378]\n",
      " [0.01553374]\n",
      " [0.00244729]\n",
      " [0.01916759]\n",
      " [0.00577867]\n",
      " [0.01322206]\n",
      " [0.00876612]\n",
      " [0.01027724]\n",
      " [0.0061554 ]\n",
      " [0.00062812]\n",
      " [0.01336281]\n",
      " [0.00606669]\n",
      " [0.00728433]\n",
      " [0.01074374]\n",
      " [0.01425787]\n",
      " [0.02286789]\n",
      " [0.01367177]\n",
      " [0.00968342]\n",
      " [0.00513467]\n",
      " [0.01597908]\n",
      " [0.00052336]\n",
      " [0.01897124]\n",
      " [0.01492095]\n",
      " [0.00900775]\n",
      " [0.00693159]\n",
      " [0.01808689]\n",
      " [0.00177696]\n",
      " [0.0039403 ]\n",
      " [0.00650679]\n",
      " [0.00756962]\n",
      " [0.01139159]\n",
      " [0.00489034]\n",
      " [0.00785567]\n",
      " [0.00830269]\n",
      " [0.0033138 ]\n",
      " [0.00156525]\n",
      " [0.01382921]\n",
      " [0.0270093 ]\n",
      " [0.03433909]\n",
      " [0.01474704]\n",
      " [0.01237182]\n",
      " [0.01184974]\n",
      " [0.02910749]\n",
      " [0.03484378]\n",
      " [0.03587579]\n",
      " [0.0242142 ]\n",
      " [0.00453124]\n",
      " [0.02017258]\n",
      " [0.00560062]\n",
      " [0.03587579]\n",
      " [0.03587579]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steffi/Documents/Udacity Kurs Deep Reinforcement Learning/ProjectBanana/UdacityNavigation/rl_lib/replay.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  priorities = np.array([experience.priority for experience in self.memory]).astype('float64')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'p' must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-183fdcd2c319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_avg_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.995\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinue_learning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Udacity Kurs Deep Reinforcement Learning/ProjectBanana/UdacityNavigation/rl_lib/training.py\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(env, brain_name, agent, n_episodes, max_t, eps_start, eps_end, eps_decay, min_avg_score, continue_learning)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# update state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Udacity Kurs Deep Reinforcement Learning/ProjectBanana/UdacityNavigation/rl_lib/agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# If enough samples are available in memory, get random subset and learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_size_of_memory_sufficient_to_draw_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Udacity Kurs Deep Reinforcement Learning/ProjectBanana/UdacityNavigation/rl_lib/agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step_local\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             delta, q_targets, q_expected = self.get_temporal_difference_error(\n",
      "\u001b[0;32m~/Documents/Udacity Kurs Deep Reinforcement Learning/ProjectBanana/UdacityNavigation/rl_lib/replay.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mbatch_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobabilites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         states = torch.from_numpy(\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'p' must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "scores = dqn(env, brain_name, agent, min_avg_score=18., eps_end=0.01, eps_decay=0.995, continue_learning=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the agent acting in the environment\n",
    "\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = agent.act(state)                      # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
